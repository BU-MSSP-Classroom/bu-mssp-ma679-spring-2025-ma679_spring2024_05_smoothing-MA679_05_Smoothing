{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b57713a5",
   "metadata": {},
   "source": [
    "# Class Workbook {.tabset .tabset-fade .tabset-pills}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae18ef1",
   "metadata": {},
   "source": [
    "## In class activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8828b95b",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from matplotlib.pyplot import subplots\n",
    "#import statsmodels.api as sm\n",
    "from plotnine import *\n",
    "import plotly.express as px\n",
    "import statsmodels.formula.api as sm\n",
    "#import ISLP as islp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b7228d",
   "metadata": {},
   "source": [
    "### Ames Housing data\n",
    "\n",
    "Let's revisit Ames Hoursing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ed864f",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "ames_raw=pd.read_csv(\"ames_raw.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e86968",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "Use data of `ames_raw` up to 2008 to predict the housing price for the later years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5ea95d",
   "metadata": {
    "Rmd_chunk_options": "echo=show_code",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "ames_raw_2009, ames_raw_2008= ames_raw.query('`Yr Sold`>=2008').copy(), ames_raw.query('`Yr Sold` <2008').copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1440d9d7",
   "metadata": {},
   "source": [
    "Use the same loss function calculator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e62215",
   "metadata": {
    "Rmd_chunk_options": "echo=show_code",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def calc_loss(prediction,actual):\n",
    "  difpred = actual-prediction\n",
    "  RMSE =pow(difpred.pow(2).mean(),1/2)\n",
    "  operation_loss=abs(sum(difpred[difpred<0]))+sum(0.1*actual[difpred>0])\n",
    "  return RMSE,operation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169cb52c",
   "metadata": {},
   "source": [
    "Use nonlinear methods discussed in ch 7 of the book.  Can you make a better prediction?\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1733e2c3",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb27fa96",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635d5412",
   "metadata": {},
   "source": [
    "### COVID Data\n",
    "\n",
    "Let's revisit COVID data.\n",
    "I've divided the data into training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc5f437",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "Train_COVID= pd.read_csv('Train_COVID.zip',compression='zip')\n",
    "Test_COVID= pd.read_csv('Test_COVID.zip',compression='zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7bd1bf",
   "metadata": {},
   "source": [
    "Try the method described in Ch7.  See if you can improve the performance.\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7950264",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169e4764",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e45dd7",
   "metadata": {},
   "source": [
    "## Problem Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b674f54",
   "metadata": {},
   "source": [
    "### College Data\n",
    "\n",
    "This question relates to the College data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a3af7f",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from ISLP import load_data\n",
    "College = load_data(\"College\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572db922",
   "metadata": {},
   "source": [
    "(a) Split the data into a training set and a test set. Using out-of-state tuition as the response and the other variables as the predictors.  Based on the exercises in the previous chapter, identify a satisfactory model that uses just a subset of the predictors.\n",
    "\n",
    "\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36517b0",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ce939e",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "\n",
    "(b) Fit a GAM on the training data, using out-of-state tuition as the response and the features selected in the previous step as the predictors. Plot the results, and explain your findings.\n",
    "\n",
    "\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc62259a",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763c2cb7",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "(c) Evaluate the model obtained on the test set, and explain the results.\n",
    "\n",
    "\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04c1715",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e04106",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "(d) For which variables, if any, is there evidence of a non-linear relationship with the response?\n",
    "\n",
    "\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f21e9fa",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe3b8ec",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a1c580",
   "metadata": {},
   "source": [
    "### Auto Data\n",
    "\n",
    "Fit some of the non-linear models investigated in chapter 7 to the `Auto` data set. Is there evidence for non-linear relationships in this data set? Create some informative plots to justify your answer.\n",
    "\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2e058",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e911a4b8",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4abe15",
   "metadata": {},
   "source": [
    "### Backfitting\n",
    "\n",
    "In Section 7.7, it was mentioned that GAMs are generally fit using\n",
    "a backfitting approach. The idea behind backfitting is actually quite\n",
    "simple. We will now explore backfitting in the context of multiple\n",
    "linear regression.\n",
    "Suppose that we would like to perform multiple linear regression, but\n",
    "we do not have software to do so. Instead, we only have software\n",
    "to perform simple linear regression. Therefore, we take the following\n",
    "iterative approach: we repeatedly hold all but one coefficient estimate\n",
    "fixed at its current value, and update only that coefficient\n",
    "estimate using a simple linear regression. The process is continued until\n",
    "convergence—that is, until the coefficient estimates stop changing.\n",
    "We now try this out on a toy example.\n",
    "\n",
    "(a) Generate a response $Y$ and two predictors $X_1$ and $X_2$, with n = 100.\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816c5a32",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62a04ef",
   "metadata": {},
   "source": [
    "(b) Write a function `simple_reg()` that takes two arguments outcome\n",
    "and feature, fits a simple linear regression model with this outcome\n",
    "and feature, and returns the estimated intercept and slope.\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e24c74",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4332a68d",
   "metadata": {},
   "source": [
    "(c) Initialize beta1 to take on a value of your choice. It does not matter what value you choose.\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70c1d99",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1624af",
   "metadata": {},
   "source": [
    "(d) Keeping beta1 fixed, use your function simple_reg() to fit the model:\n",
    "$Y-beta_1X_1=\\beta_0+\\beta_2X_2+\\epsilon$\n",
    "Store the resulting values as beta0 and beta2.\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a00276b",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ac05ad",
   "metadata": {},
   "source": [
    "(e) Keeping beta2 fixed, fit the model\n",
    "$Y-beta_2X_2=\\beta_0+\\beta_1X_1+\\epsilon$\n",
    "Store the result as beta0 and beta1 (overwriting their previous values).\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8ea4c6",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76a0098",
   "metadata": {},
   "source": [
    "(f) Write a for loop to repeat (c) and (d) 1,000 times. Report the\n",
    "estimates of beta0, beta1, and beta2 at each iteration of the for\n",
    "loop. Create a plot in which each of these values is displayed,\n",
    "with beta0, beta1, and beta2.\n",
    "\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ed2b63",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24de57b8",
   "metadata": {},
   "source": [
    "(g) Compare your answer in (e) to the results of simply performing\n",
    "multiple linear regression to predict $Y$ using $X_1$ and $X_2$.\n",
    "Use `axline()` method to overlay those multiple linear regression\n",
    "coefficient estimates on the plot obtained in (e).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bb0e31",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d921cd",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "\n",
    "(h) On this data set, how many backfitting iterations were required\n",
    "in order to obtain a “good” approximation to the multiple regression\n",
    "coefficient estimates?\n",
    "\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e73d15",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa4e942",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "\n",
    "(h) In a toy example with `p = 100`, show that one can approximate the multiple linear regression coefficient estimates by repeatedly performing simple linear regression in a backfitting procedure. How many backfitting iterations are required in order to obtain a \"good\" approximation to the multiple regression coefficient estimates? Create a plot to justify your answer.\n",
    "\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90401ab1",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e1613b",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5852613",
   "metadata": {},
   "source": [
    "### Boston Data\n",
    "\n",
    "This question uses the variables dis (the weighted mean of distances to five Boston employment centers) and nox (nitrogen oxides concentration in parts per 10 million) from the Boston data. We will treat dis as the predictor and nox as the response.\n",
    "(a) Use the `poly()` function from the `ISLP.models` module to fit a cubic polynomial regression to predict nox using dis. Report the regression output, and plot the resulting data and polynomial fits.\n",
    "\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640ec663",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b19553",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "(b) Plot the polynomial fits for a range of different polynomial degrees (say, from 1 to 10), and report the associated residual sum of squares.\n",
    "\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8b52d9",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff097b7",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "(c) Perform cross-validation or another approach to select the optimal degree for the polynomial, and explain your results.\n",
    "\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739fdc66",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853c181b",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "(d) Use the `bs()` function from the `ISLP.models` module to fit a regression\n",
    "spline to predict nox using dis. Report the output for\n",
    "the fit using four degrees of freedom. How did you choose the\n",
    "knots? Plot the resulting fit.\n",
    "\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68af2c5a",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d4d357",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "(e) Now fit a regression spline for a range of degrees of freedom, and\n",
    "plot the resulting fits and report the resulting RSS. Describe the\n",
    "results obtained.\n",
    "\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eb53ba",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329fd90c",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "(f) Perform cross-validation or another approach in order to select the best degrees of freedom for a regression spline on this data.\n",
    "Describe your results.\n",
    "\n",
    "\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7960beb8",
   "metadata": {
    "Rmd_chunk_options": "echo=TRUE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b2753c",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "~~~\n",
    "Please write your answer in full sentences.\n",
    "\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf7a3fa",
   "metadata": {},
   "source": [
    "## Additional Material"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77488bfa",
   "metadata": {},
   "source": [
    "### Local regression\n",
    "\n",
    "Starting with one predictor, we want to estimate a function $f$ that describes the outcome of interest $y$ such that\n",
    "$$y=f(x)+\\epsilon$$\n",
    "where $\\epsilon$ is some error.\n",
    "\n",
    "In linear regression, we assumed the functional form to be a line\n",
    "$$f(x)=\\alpha+\\beta x$$\n",
    "where $\\alpha$ is the intercept and $\\beta$ is the slope. This simplification allowed us to summarize the relationship between the two variables using just one number $\\beta$.  In reality, this is a gross simplification, and in particular, when prediction accuracy is more important than the description of the trend in the relationship, we may want to use more flexible methods.\n",
    "\n",
    "For example, if your data looks like the following.  What do you do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87be8c8b",
   "metadata": {
    "Rmd_chunk_options": "echo=FALSE, fig.width=10,fig.height=4 ,out.width=\"90%\",message=FALSE",
    "jupyter": {
     "output_hidden": false,
     "source_hidden": true
    },
    "kernel": "Python3",
    "tags": [
     "report_output"
    ]
   },
   "outputs": [],
   "source": [
    "# example based on\n",
    "# http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch04.pdf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "\n",
    "np.random.seed(42)\n",
    "# Generate data\n",
    "x = np.random.uniform(0, 3, 300)\n",
    "yf = np.sin(x) * np.cos(20 * x) + np.random.normal(0, 0.15, len(x))\n",
    "yg = np.log(x**2 - x + 1) + np.random.normal(0, 0.15, len(x))\n",
    "\n",
    "# Create a DataFrame\n",
    "dt = pd.DataFrame({'x': x, 'yf': yf, 'yg': yg})\n",
    "dt = dt.sort_values(by='x')\n",
    "dt = dt.reset_index()\n",
    "def afun(x):\n",
    "    return np.sin(x) * np.cos(20 * x)\n",
    "\n",
    "\n",
    "(\n",
    "  ggplot(dt)+geom_point()+aes(x=\"x\",y=\"yf\")+\n",
    "    stat_function(fun=afun,color=\"blue\",alpha=0.7)\n",
    ")\n",
    "\n",
    "def bfun(x):\n",
    "    return np.log((x)**2 - x + 1)\n",
    "\n",
    "\n",
    "(\n",
    "  ggplot(dt)+geom_point()+aes(x=x,y=yg)+\n",
    "  stat_function(fun=bfun,color=\"blue\",alpha=0.7)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9277f3c",
   "metadata": {},
   "source": [
    "clearly, linear relation does not hold globally.  Even in such cases, we could look at local relationships and then sew them together to get a global picture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2397d6",
   "metadata": {},
   "source": [
    "#### Running means or the nearest neighbor\n",
    "\n",
    "The simplest way to estimate $f$ at $x_i$ locally is by averaging the $y$s corresponding to $x$s near $x_i$.\n",
    "$$\\hat{f}(x_i)=\\sum_{j\\in N(x_i)} y_j / n_i$$\n",
    "where $N(x_i)$ indexes $n_i$ neighbors of $x_i$.\n",
    "\n",
    "$N(x_i)$ can be defined as you wish, but the most popular choice is to use\n",
    " a symmetric neighborhood consisting of the nearest $2k + 1$ points:\n",
    "\n",
    "$$N(x_i) = { max(i-k, 1), \\dots, i-1, i, i + 1,\\dots, min(i+k, n) }.$$\n",
    "\n",
    "For example, if we were to use the 10 closest points so that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36792a2e",
   "metadata": {
    "Rmd_chunk_options": "echo=FALSE, fig.width=10,fig.height=4 ,out.width=\"90%\",message=FALSE",
    "jupyter": {
     "output_hidden": false,
     "source_hidden": true
    },
    "kernel": "Python3",
    "tags": [
     "report_output"
    ]
   },
   "outputs": [],
   "source": [
    "# x = np.random.uniform(0, 3, 300)\n",
    "# yf = np.sin(x) * np.cos(20 * x) + np.random.normal(0, 0.15, len(x))\n",
    "# yg = np.log(x**2 - x + 1) + np.random.normal(0, 0.15, len(x))\n",
    "#\n",
    "# # Create a DataFrame\n",
    "# dt = pd.DataFrame({'x': x, 'yf': yf, 'yg': yg})\n",
    "# dt = dt.sort_values(by='x')\n",
    "\n",
    "# Create colors\n",
    "colors = np.repeat('black', 300)\n",
    "colors[144:155]='grey'\n",
    "#np.where(np.arange(1, 301) % np.arange(1, 301)[:, None] in range(145, 156), 'black', 'grey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e55798",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "(\n",
    "  ggplot(dt)+geom_point(color=colors)+aes(x=\"x\",y=\"yf\")\n",
    "+geom_point(aes(x=dt['x'][149],y=dt['yf'][144:155].mean()),color=\"red\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea92de7f",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "(\n",
    "  ggplot(dt)+geom_point(color=colors)+aes(x=\"x\",y=\"yg\")\n",
    "+geom_point(aes(x=dt['x'][149],y=dt['yg'][144:155].mean()),color=\"red\")\n",
    ")\n",
    "\n",
    "# # Plotting\n",
    "# plt.figure(figsize=(12, 6))\n",
    "#\n",
    "# # Plot for yf\n",
    "# plt.subplot(1, 2, 1)\n",
    "# sns.scatterplot(x='x', y='yf', data=dt, hue=colors)\n",
    "# sns.scatterplot(x=dt['x'][149], y=dt['yf'][144:155].mean(), color='red', label='Mean', s=100)\n",
    "#\n",
    "# # Plot for yg\n",
    "# plt.subplot(1, 2, 2)\n",
    "# sns.scatterplot(x='x', y='yg', data=dt, hue=colors)\n",
    "# sns.scatterplot(x=dt['x'][149], y=dt['yg'][144:155].mean(), color='red', label='Mean', s=100)\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176fa547",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#colors=ifelse(1:300%in%(145:155),\"black\",\"grey\")\n",
    "# gridExtra::grid.arrange(\n",
    "#   ggplot(dt)+geom_point(color=colors)+aes(x=x,y=yf)+geom_point(aes(x=dt$x[150],y=mean(dt$yf[145:155])),color=\"red\"),\n",
    "# ggplot(dt)+geom_point(color=colors)+aes(x=x,y=yg)+geom_point(aes(x=dt$x[150],y=mean(dt$yg[145:155])),color=\"red\")\n",
    "# ,ncol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6889179a",
   "metadata": {},
   "source": [
    "If we keep repeating the procedure for each point and connect the dots together, we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a371140",
   "metadata": {
    "Rmd_chunk_options": "echo=FALSE, fig.width=10,fig.height=4 ,out.width=\"90%\",message=FALSE,warning=FALSE",
    "jupyter": {
     "output_hidden": false,
     "source_hidden": true
    },
    "kernel": "Python3",
    "tags": [
     "report_output"
    ]
   },
   "outputs": [],
   "source": [
    "dt[\"rolling_mean_yf\"]=dt[\"yf\"].rolling(10).mean()\n",
    "dt[\"rolling_mean_yg\"]=dt[\"yg\"].rolling(10).mean()\n",
    "\n",
    "(\n",
    "  ggplot(dt)+geom_point(color=\"gray\")+aes(x=\"x\",y=\"yf\")+geom_line(aes(x=\"x\",y=\"rolling_mean_yf\"),color=\"red\")\n",
    ")\n",
    "(\n",
    "ggplot(dt)+geom_point(color=\"gray\")+aes(x=\"x\",y=\"yg\")+geom_line(aes(x=\"x\",y=\"rolling_mean_yg\"),color=\"red\")+geom_point(aes(x=dt[\"x\"].iloc[150],y=dt[\"yg\"].iloc[145:155].mean()),color=\"red\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc349693",
   "metadata": {},
   "source": [
    "It looks surprisingly well on the left, but it's clearly too wigly on the right.  Usually ends are pretty bad and we can't use points at the ends that do not have observations.\n",
    "\n",
    "If we increase the neighborhood size to 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05792682",
   "metadata": {
    "Rmd_chunk_options": "echo=FALSE, fig.width=10,fig.height=4 ,out.width=\"90%\",message=FALSE,warning=FALSE",
    "jupyter": {
     "output_hidden": false,
     "source_hidden": true
    },
    "kernel": "Python3",
    "tags": [
     "report_output"
    ]
   },
   "outputs": [],
   "source": [
    "dt[\"rolling_mean_yf\"]=dt[\"yf\"].rolling(20).mean()\n",
    "dt[\"rolling_mean_yg\"]=dt[\"yg\"].rolling(20).mean()\n",
    "\n",
    "(\n",
    "  ggplot(dt)+geom_point(color=\"gray\")+aes(x=\"x\",y=\"yf\")+geom_line(aes(x=\"x\",y=\"rolling_mean_yf\"),color=\"red\")\n",
    ")\n",
    "(\n",
    "ggplot(dt)+geom_point(color=\"gray\")+aes(x=\"x\",y=\"yg\")+geom_line(aes(x=\"x\",y=\"rolling_mean_yg\"),color=\"red\")+geom_point(aes(x=dt[\"x\"].iloc[150],y=dt[\"yg\"].iloc[145:155].mean()),color=\"red\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b4c1d9",
   "metadata": {},
   "source": [
    "Now the left side is too smooth and the right side looks more decent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78ce2f6",
   "metadata": {},
   "source": [
    "### Running Line\n",
    "Instead of using a simple mean, we can fit a regression at each neighborhood.  Then make a prediction at each $x_i$ so that\n",
    "$$\\hat{f} ( x_i ) = \\hat{\\alpha}_i + \\hat{\\beta}_i x_i ,$$\n",
    "where $\\hat{\\alpha}_i$ and $\\hat{\\beta}_i$ are OLS estimates based on points in a neighborhood $N(x_i)$ of $x_i$. This is actually easy to do thanks to well-known regression updating formulas. Extension to weighted data is obvious. It's much better than running means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffc8d71",
   "metadata": {
    "Rmd_chunk_options": "echo=FALSE, fig.width=10,fig.height=4 ,out.width=\"90%\",message=FALSE,warning=FALSE",
    "jupyter": {
     "output_hidden": false,
     "source_hidden": true
    },
    "kernel": "Python3",
    "tags": [
     "report_output"
    ]
   },
   "outputs": [],
   "source": [
    "from statsmodels.regression.rolling import RollingOLS\n",
    "model1 = RollingOLS(endog =dt['yf'].values , exog=dt[['x']],window=10)\n",
    "rres1 = model1.fit()\n",
    "\n",
    "model2 = RollingOLS(endog =dt['yg'].values , exog=dt[['x']],window=10)\n",
    "rres2 = model2.fit()\n",
    "\n",
    "dt['predicted_yf10'] = np.nan\n",
    "dt['predicted_yg10'] = np.nan\n",
    "for i in range(0, len(dt)):\n",
    "    dt.loc[i,\"predicted_yf10\"] = np.dot(rres1.params.iloc[i], dt[['x']].iloc[i])\n",
    "    dt.loc[i,\"predicted_yg10\"] = np.dot(rres2.params.iloc[i], dt[['x']].iloc[i])\n",
    "\n",
    "(\n",
    "  ggplot(dt)+geom_point(color=\"gray\")+aes(x=\"x\",y=\"yf\")+geom_line(aes(x=\"x\",y=\"predicted_yf10\"),color=\"red\")\n",
    ")\n",
    "(\n",
    "ggplot(dt)+geom_point(color=\"gray\")+aes(x=x,y=yg)+geom_line(aes(x=\"x\",y=\"predicted_yg10\"),color=\"red\")  +geom_point(aes(x=dt[\"x\"].iloc[150],y=dt[\"yg\"].iloc[145:155].mean()),color=\"red\")\n",
    ")\n",
    "\n",
    "\n",
    "#library(zoo)\n",
    "#\n",
    "# from statsmodels.regression.rolling import RollingOLS\n",
    "# roll_reg = RollingOLS(dt['yf'],dt['x'], window=11)\n",
    "# model = roll_reg.fit()\n",
    "# fig = model.plot_recursive_coefficient(variables=['x'])\n",
    "# plt.xlabel('Time step')\n",
    "# plt.ylabel('Coefficient value')\n",
    "# plt.show()\n",
    "#\n",
    "# from scipy.stats import linregress\n",
    "#\n",
    "# # Assuming dt is a DataFrame containing 'x', 'yf', and 'yg' columns\n",
    "#\n",
    "# # Define a function to extract the slope coefficient from linear regression\n",
    "# def extract_slope(df):\n",
    "#     slope, _, _, _, _ = linregress(df['x'], df['yf'])\n",
    "#     return slope\n",
    "#\n",
    "# # Apply rolling window and extract the slope coefficient\n",
    "# Coeff = dt.rolling(window=11, center=True).apply(extract_slope, raw=False)\n",
    "#\n",
    "# # Define a function to extract the slope coefficient for yg\n",
    "# def extract_slope_yg(df):\n",
    "#     slope, _, _, _, _ = linregress(df['x'], df['yg'])\n",
    "#     return slope\n",
    "#\n",
    "# # Apply rolling window and extract the slope coefficient for yg\n",
    "# Coefg = dt.rolling(window=11, center=True).apply(extract_slope_yg, raw=False)\n",
    "#\n",
    "# Coeff <- zoo::rollapply(dt,\n",
    "#           width = 11, by=1,\n",
    "#           FUN = function(z) {\n",
    "#             predict(lm(yf~x, data=as.data.frame(z)))[6]\n",
    "#           },\n",
    "#           by.column = FALSE, align = \"center\",fill = NA )\n",
    "# Coefg <- zoo::rollapply(dt,\n",
    "#           width = 11, by=1,\n",
    "#           FUN = function(z) {\n",
    "#             predict(lm(yg~x, data=as.data.frame(z)))[6]\n",
    "#           },\n",
    "#           by.column = FALSE, align = \"center\",fill = NA )\n",
    "# gridExtra::grid.arrange(\n",
    "#   ggplot(dt)+geom_point(color=\"gray\")+aes(x=x,y=yf)+geom_line(aes(x=dt$x,y=Coeff),color=\"red\"),\n",
    "# ggplot(dt)+geom_point(color=\"gray\")+aes(x=x,y=yg)+geom_point(aes(x=dt$x[150],y=mean(dt$yg[145:155])),color=\"red\")+geom_line(aes(x=dt$x,y=Coefg),color=\"red\")\n",
    "# ,ncol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1678ae09",
   "metadata": {},
   "source": [
    "If we were to increase the neighborhood size to 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5668c3a0",
   "metadata": {
    "Rmd_chunk_options": "echo=FALSE, fig.width=10,fig.height=4 ,out.width=\"90%\",message=FALSE,warning=FALSE",
    "jupyter": {
     "output_hidden": false,
     "source_hidden": true
    },
    "kernel": "Python3",
    "tags": [
     "report_output"
    ]
   },
   "outputs": [],
   "source": [
    "from statsmodels.regression.rolling import RollingOLS\n",
    "model1 = RollingOLS(endog =dt['yf'].values , exog=dt[['x']],window=20)\n",
    "rres1 = model1.fit()\n",
    "\n",
    "model2 = RollingOLS(endog =dt['yg'].values , exog=dt[['x']],window=20)\n",
    "rres2 = model2.fit()\n",
    "\n",
    "dt['predicted_yf'] = np.nan\n",
    "dt['predicted_yg'] = np.nan\n",
    "for i in range(0, len(dt)):\n",
    "    dt.loc[i,\"predicted_yf\"] = np.dot(rres1.params.iloc[i], dt[['x']].iloc[i])\n",
    "    dt.loc[i,\"predicted_yg\"] = np.dot(rres2.params.iloc[i], dt[['x']].iloc[i])\n",
    "\n",
    "\n",
    "\n",
    "(\n",
    "  ggplot(dt)+geom_point(color=\"gray\")+aes(x=\"x\",y=\"yf\")+geom_line(aes(x=\"x\",y=\"predicted_yf\"),color=\"red\")\n",
    ")\n",
    "(\n",
    "ggplot(dt)+geom_point(color=\"gray\")+aes(x=x,y=yg)+geom_line(aes(x=\"x\",y=\"predicted_yg\"),color=\"red\")  +geom_point(aes(x=dt[\"x\"].iloc[150],y=dt[\"yg\"].iloc[145:155].mean()),color=\"red\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab831f1",
   "metadata": {},
   "source": [
    "### Weighted running means (Kernel Smoothers)\n",
    "An alternative approach is to use a weighted running mean, with weights that decline as one moves away from the target value. To calculate $\\hat{f}(x_i)$, the $j$th point receives weight\n",
    "$$w_{ij}=\\frac{c_i}{\\lambda}d\\left(\\frac{|x_i-x_j|}{\\lambda}\\right)$$,\n",
    "\\begin{itemize}\n",
    "\\item $\\lambda$ is a tunning constant called the window width or bandwidth\n",
    "\\item $c_i$ is a normalizing constant so the weights add up to one for each $x_i$\n",
    "\\item $d()$ is any kernel such as\n",
    "\\begin{itemize}\n",
    "\\item Gaussian density,\n",
    "\\item Epanechnikov:\n",
    "\\begin{eqnarray*}\n",
    "d(t) =\n",
    "\\begin{cases}\n",
    "    3/4 (1 -t^2) & \\text{if $t^2 < 1$} \\\\\n",
    "    0 & \\text{other wise}\n",
    "  \\end{cases}\n",
    "\\end{eqnarray*}\n",
    "\\item Minimum var:\n",
    "\\begin{eqnarray*}\n",
    "d(t) =\n",
    "\\begin{cases}\n",
    "    3/8 (3 -5t^2) & \\text{if $t^2 < 3/5$} \\\\\n",
    "    0 & \\text{other wise}\n",
    "  \\end{cases}\n",
    "\\end{eqnarray*}\n",
    "\\item tri-cube:\n",
    "\\begin{eqnarray*}\n",
    "d(t) =\n",
    "\\begin{cases}\n",
    "   70/81 (1-|t|^3)^3 & \\text{if $0 \\leq t \\leq 1$} \\\\\n",
    "    0 & \\text{other wise}\n",
    "  \\end{cases}\n",
    "\\end{eqnarray*}\n",
    "\\end{itemize}\n",
    "\\end{itemize}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21938b7c",
   "metadata": {
    "Rmd_chunk_options": "echo=FALSE, fig.width=12,fig.height=3 ,out.width=\"90%\",message=FALSE",
    "jupyter": {
     "output_hidden": false,
     "source_hidden": true
    },
    "kernel": "Python3",
    "tags": [
     "report_output"
    ]
   },
   "outputs": [],
   "source": [
    "# Define the Gaussian kernel\n",
    "def gaussian_kernel(x):\n",
    "    return np.exp(-0.5 * x**2) / np.sqrt(2 * np.pi)\n",
    "\n",
    "# Define the Epanechnikov kernel\n",
    "def epanechnikov_kernel(x, lambda_=1):\n",
    "    return np.where((np.abs(x) / lambda_)**2 < 1, (1 / lambda_) * (3 / 4) * (1 - (np.abs(x) / lambda_)**2), 0)\n",
    "\n",
    "# Define the Minimum Variance kernel\n",
    "def minimum_variance_kernel(x, lambda_=1):\n",
    "    return np.where((np.abs(x) / lambda_)**2 < 3/5, (1 / lambda_) * (3 / 8) * (3 - 5 * (np.abs(x) / lambda_)**2), 0)\n",
    "\n",
    "# Define the Tri-cube kernel\n",
    "def tricube_kernel(x, lambda_=1):\n",
    "    return np.where((np.abs(x) / lambda_) < 1, (70 / 81) * (1 - np.abs(x)**3)**3, 0)\n",
    "\n",
    "# Create x values for plotting\n",
    "x = np.linspace(-3, 3, 1000)\n",
    "\n",
    "# Set up subplots\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "# Plot each kernel function\n",
    "axes[0].plot(x, gaussian_kernel(x))\n",
    "axes[0].set_title('Gaussian')\n",
    "axes[1].plot(x, epanechnikov_kernel(x))\n",
    "axes[1].set_title('Epanechnikov')\n",
    "axes[2].plot(x, minimum_variance_kernel(x))\n",
    "axes[2].set_title('Minimum Variance')\n",
    "axes[3].plot(x, tricube_kernel(x))\n",
    "axes[3].set_title('Tri-cube')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c348a2",
   "metadata": {},
   "source": [
    "As you can see from the figures, some Kernels are local because there is a border where things don't matter.  Whereas Gaussian kernel is an example of a global Kernel because all the points contribute.\n",
    "\n",
    "Using Epanechnikov kernel with lambda 0.05 and 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4013c6ef",
   "metadata": {
    "Rmd_chunk_options": "echo=FALSE, fig.width=10,fig.height=4 ,out.width=\"80%\",message=FALSE",
    "jupyter": {
     "output_hidden": false,
     "source_hidden": true
    },
    "kernel": "Python3",
    "tags": [
     "report_output"
    ]
   },
   "outputs": [],
   "source": [
    "# w=outer(dt$x,dt$x,function(x,y,lambda=0.05) {ifelse((abs(x-y)/lambda)^2<1,(1/lambda)*(3/4)*(1-(abs(x-y)/lambda)^2),0)})\n",
    "# w=w/rowSums(w)\n",
    "# w2=outer(dt$x,dt$x,function(x,y,lambda=0.2) {ifelse((abs(x-y)/lambda)^2<1,(1/lambda)*(3/4)*(1-(abs(x-y)/lambda)^2),0)})\n",
    "# w2=w2/rowSums(w2)\n",
    "\n",
    "# Define the Epanechnikov kernel function\n",
    "def epanechnikov_kernel(x, y, lambda_=0.05):\n",
    "    return np.where((np.abs(x - y) / lambda_)**2 < 1, (1 / lambda_) * (3 / 4) * (1 - (np.abs(x - y) / lambda_)**2), 0)\n",
    "\n",
    "# Generate weight matrix w\n",
    "w = np.array([[epanechnikov_kernel(x, y) for y in dt['x']] for x in dt['x']])\n",
    "w = w / np.sum(w, axis=1, keepdims=True)\n",
    "\n",
    "# Generate weight matrix w2\n",
    "w2 = np.array([[epanechnikov_kernel(x, y, lambda_=0.2) for y in dt['x']] for x in dt['x']])\n",
    "w2 = w2 / np.sum(w2, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "dt[\"wyf\"] =w@dt[\"yf\"]\n",
    "dt[\"wyg\"] =w@dt[\"yg\"]\n",
    "(\n",
    "  ggplot(dt)+geom_point(color=\"gray\")+aes(x=\"x\",y=\"yf\")+geom_line(aes(x=\"x\",y=\"wyf\"),color=\"red\")\n",
    ")\n",
    "(\n",
    "ggplot(dt)+geom_point(color=\"gray\")+aes(x=\"x\",y=\"yg\")+geom_point(aes(x=dt[\"x\"].iloc[150],y=dt[\"yg\"].iloc[145:155].mean()),color=\"red\")+geom_line(aes(x=\"x\",y=\"wyg\"),color=\"red\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f843640",
   "metadata": {},
   "source": [
    "### Loess/Lowess\n",
    "\n",
    "- Loess in nutshell is a locally weighted running line smoother.\n",
    "- It is the default smoother in ggplot when you call `geom_smooth`.\n",
    "- To calculate $\\hat{f}(x_i)$\n",
    "\\begin{itemize}\n",
    "\\item find a symmetric nearest neighborhood of $x_i$,\n",
    "\\item find the distance from $x_i$ to the furthest neighbor and use this as $\\lambda$,\n",
    "\\item use a tri-cube weight function\n",
    "\\begin{eqnarray*}\n",
    "d(t) =\n",
    "\\begin{cases}\n",
    "   70/81 (1-|t|^3)^3 & \\text{if $0 \\leq t \\leq 1$} \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "\\end{eqnarray*}\n",
    "\\item estimate a local line using these weights, take the fitted value at $x_i$ as $\\hat{f}(x_i)$.\n",
    "\\end{itemize}\n",
    "\n",
    "A variant uses robust regression in each neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7e700d",
   "metadata": {
    "Rmd_chunk_options": "echo=FALSE, fig.width=10,fig.height=4 ,out.width=\"80%\",message=FALSE",
    "jupyter": {
     "output_hidden": false,
     "source_hidden": true
    },
    "kernel": "Python3",
    "tags": [
     "report_output"
    ]
   },
   "outputs": [],
   "source": [
    "#pip install scikit-misc\n",
    "\n",
    "(\n",
    "  ggplot(dt)+geom_point(color=\"gray\")+aes(x=\"x\",y=\"yf\")+geom_smooth(method=\"loess\",se=False)\n",
    ")\n",
    "(\n",
    "  ggplot(dt)+geom_point(color=\"gray\")+aes(x=\"x\",y=\"yg\")+geom_smooth(method=\"loess\",se=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a934e9a3",
   "metadata": {},
   "source": [
    "By default setting loess did not work well on the left example.  To take into account the small bandwidth you need to specify the span option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d7cec1",
   "metadata": {
    "Rmd_chunk_options": "echo=FALSE, fig.width=10,fig.height=9 ,out.width=\"80%\",message=FALSE",
    "jupyter": {
     "output_hidden": false,
     "source_hidden": true
    },
    "kernel": "Python3",
    "tags": [
     "report_output"
    ]
   },
   "outputs": [],
   "source": [
    "(\n",
    "  ggplot(dt)+geom_point(color=\"gray\")+aes(x=\"x\",y=\"yf\")+geom_smooth(method=\"loess\",span=0.3,se=False)\n",
    ")\n",
    "(\n",
    "  ggplot(dt)+geom_point(color=\"gray\")+aes(x=\"x\",y=\"yf\")+geom_smooth(method=\"loess\",span=0.1,se=False)\n",
    ")\n",
    "# (\n",
    "#   ggplot(dt)+geom_point(color=\"gray\")+aes(x=\"x\",y=\"yf\")+geom_smooth(method=\"loess\",span=0.05,se=False)\n",
    "# )\n",
    "# (\n",
    "#     ggplot(dt)+geom_point(color=\"gray\")+aes(x=\"x\",y=\"yf\")+geom_smooth(method=\"loess\",span=0.03,se=False)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7152f571",
   "metadata": {},
   "source": [
    "### Other useful functions\n",
    "\n",
    "Scatter plot smoothing with loess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1899d2",
   "metadata": {
    "Rmd_chunk_options": "fig.width=6,fig.height=5 ,out.width=\"60%\",message=FALSE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from ISLP import load_data\n",
    "Auto = load_data('Auto')\n",
    "# with(Auto, scatter.smooth(horsepower, mpg, lpars =\n",
    "#                     list(col = \"red\", lwd = 3, lty = 3)))\n",
    "\n",
    "sns.regplot(x=\"horsepower\",y=\"mpg\",data=Auto, lowess=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd79052e",
   "metadata": {},
   "source": [
    "2D Kernel density estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62db73b6",
   "metadata": {
    "Rmd_chunk_options": "fig.width=6,fig.height=5 ,out.width=\"60%\",message=FALSE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# n <- 10000\n",
    "# x1  <- matrix(rnorm(n), ncol = 2)\n",
    "# x2  <- matrix(rnorm(n, mean = 3, sd = 1.5), ncol = 2)\n",
    "# x   <- rbind(x1, x2)\n",
    "# n = 10000\n",
    "\n",
    "# Generating data\n",
    "n = 10000\n",
    "x1 = np.random.normal(size=(n, 2))\n",
    "x2 = np.random.normal(loc=3, scale=1.5, size=(n, 2))\n",
    "x = np.concatenate((x1, x2), axis=0)\n",
    "dtx = pd.DataFrame({'x1': x[:,0], 'x2': x[:,1]})\n",
    "sns.kdeplot(data=dtx,x=\"x1\",y=\"x2\",fill=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951458ae",
   "metadata": {},
   "source": [
    "### Splines\n",
    "\n",
    "Splines are used for interpolation and smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7277fa",
   "metadata": {
    "Rmd_chunk_options": "echo=FALSE, fig.width=10,fig.height=4 ,out.width=\"90%\",message=FALSE",
    "jupyter": {
     "output_hidden": false,
     "source_hidden": true
    },
    "kernel": "Python3",
    "tags": [
     "report_output"
    ]
   },
   "outputs": [],
   "source": [
    "# ggplot(cars)+geom_point()+aes(x=speed,y=dist)+\n",
    "#   geom_smooth(method=\"lm\",color=\"blue\")+\n",
    "#   geom_smooth(method=\"loess\",color=\"red\")\n",
    "# cited example from http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch04.pdf\n",
    "# x = runif(300,0,3)\n",
    "# yf = sin(x)*cos(20*x)+rnorm(length(x),0,0.15)\n",
    "# yg = log((x)^2-x+1)+rnorm(length(x),0,0.15)\n",
    "# dt<-data.frame(x,yf,yg)\n",
    "# dt<- dt[order(x),]\n",
    "# n <- 9\n",
    "# xi <- 1:n\n",
    "# yi <- rnorm(n)\n",
    "# gridExtra::grid.arrange(\n",
    "# ggplot(data.frame(xi,yi))+geom_point()+aes(x=xi,y=yi),\n",
    "# ggplot(dt)+geom_point()+aes(x=x,y=yg)+stat_function(fun=function(x)log((x)^2-x+1),color=\"blue\",alpha=0.7)\n",
    "# ,ncol=2)\n",
    "\n",
    "\n",
    "# np.random.seed(123)  # for reproducibility\n",
    "# x = np.random.uniform(0, 3, 300)\n",
    "# yf = np.sin(x) * np.cos(20 * x) + np.random.normal(0, 0.15, len(x))\n",
    "# yg = np.log((x)**2 - x + 1) + np.random.normal(0, 0.15, len(x))\n",
    "#\n",
    "# # Creating a DataFrame\n",
    "# dt = pd.DataFrame({'x': x, 'yf': yf, 'yg': yg})\n",
    "# dt = dt.sort_values(by='x')  # Sorting by x\n",
    "\n",
    "# Creating data for grid.arrange\n",
    "n = 9\n",
    "xi = np.arange(1, n+1)\n",
    "yi = np.random.normal(size=n)\n",
    "\n",
    "dti = pd.DataFrame({'xi': xi, 'yi': yi})\n",
    "(\n",
    "  ggplot(dti)+geom_point()+aes(x=\"xi\",y=\"yi\"),\n",
    ")\n",
    "\n",
    "(\n",
    "  ggplot(dt)+geom_point()+aes(x=\"x\",y=\"yg\")+stat_function(fun=lambda x: np.log((x)**2-x+1),color=\"blue\",alpha=0.7)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4314753",
   "metadata": {},
   "source": [
    "On the left, we see 9 points observed across the span of $x$, and the goal is to fit a line through these points, not necessarily find a linear trend.  On the left is a case where we see a bend in the data and the idea is to find a reasonable line representing this data cloud.\n",
    "\n",
    "The initial starting point is to fit a straight line and glue them together. We can do so by fitting regression to subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d40b39",
   "metadata": {
    "Rmd_chunk_options": "echo=FALSE, fig.width=10,fig.height=4 ,out.width=\"80%0%\",message=FALSE",
    "jupyter": {
     "output_hidden": false,
     "source_hidden": true
    },
    "kernel": "Python3",
    "tags": [
     "report_output"
    ]
   },
   "outputs": [],
   "source": [
    "dt['co'] = pd.factorize(1 * (dt['x'] > 0.5) + 1)[0] + 1\n",
    "(\n",
    "ggplot(dti)+geom_point()+geom_line()+aes(x=\"xi\",y=\"yi\")\n",
    ")\n",
    "(\n",
    "ggplot(dt)+geom_point(alpha=0.2)+aes(x=\"x\",y=\"yg\",color=\"factor(co)\")+geom_smooth(method=\"lm\",se=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0836e7",
   "metadata": {},
   "source": [
    "The problem, as apparent from the figure above, is that the result is not necessarily smooth or continuous where the lines meet, which is called the knots.  Spline is a popular method to achieve continuity at the knots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cc371d",
   "metadata": {},
   "source": [
    "### Splines\n",
    "\n",
    "A spline is a piece-wise polynomial with pieces defined by a sequence of knots\n",
    "$$\\eta_1 <\\eta_2<\\cdots<\\eta_k$$\n",
    "such that the pieces join smoothly at the knots.\n",
    "\n",
    "The simplest case is a linear spline with one knot $\\eta_1$\n",
    "$$S(x)=\\beta_0+\\beta_1 x +\\gamma ( x-\\eta_1)_{+}$$\n",
    "term $( x - \\eta_1)_{+}$ is 0 until $x$ is larger than $\\eta_1$.\n",
    "<!-- Which means up to $\\eta_1$, $S(x)=\\beta_0+\\beta_1 x$ and after $\\eta_1$    -->\n",
    "<!-- $S(x)=a(\\beta_0 - \\gamma\\eta_1)+(\\beta_1+\\gamma) x$.   -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18362a2",
   "metadata": {
    "Rmd_chunk_options": "echo=FALSE, fig.width=5,fig.height=4 ,out.width=\"80%\",message=FALSE",
    "jupyter": {
     "output_hidden": false,
     "source_hidden": true
    },
    "kernel": "Python3",
    "tags": [
     "report_output"
    ]
   },
   "outputs": [],
   "source": [
    "## Fit model\n",
    "\n",
    "# lm1 <- lm(formula = yg ~ bs(x, df = NULL, knots = c(0.5), degree = 1),\n",
    "#           data    = dt)\n",
    "# ## Create a data frame to hold prediction\n",
    "# newdat <- data.frame(x = seq(from = min(dt$x),\n",
    "#                                  to = max(dt$x), by = 0.01))\n",
    "# ## Predict\n",
    "# newdat$yg <- predict(lm1, newdata = newdat)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#Fit the linear regression model\n",
    "lm1 = sm.formula.ols(formula='yg ~ bs(x, knots=[0.5], degree=1)', data=dt).fit()\n",
    "\n",
    "# Create a data frame for prediction\n",
    "newdat = pd.DataFrame({'x': np.arange(dt['x'].min(), dt['x'].max(), 0.01)})\n",
    "\n",
    "# Predict\n",
    "newdat['yg'] = lm1.predict(newdat)\n",
    "\n",
    "## Plot the previous plot with a regression line\n",
    "(\n",
    "  ggplot(data = dt,\n",
    "                mapping = aes(x = \"x\", y = \"yg\")) +geom_point(alpha=0.3)+ geom_line(data = newdat, color = \"red\")+stat_function(fun=lambda x: np.log((x)**2-x+1),color=\"blue\",alpha=0.7,linetype=\"dashed\")\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6487ca1f",
   "metadata": {},
   "source": [
    "For a spline of degree $m$ one usually requires the polynomials and their\n",
    "first $m - 1$ derivatives to agree at the knots, so that $m - 1$ derivatives are continuous.\n",
    "A spline of degree $m$ can be represented as a power series:\n",
    "\n",
    "$$S(x)=\\sum^m_{j=0}\\beta_0 x^j+\\sum^k_{j=1}\\gamma_j(x-\\eta_j)^{m}_{+}$$\n",
    "\n",
    "The most popular splines are cubic splines:\n",
    "$$S(x)= \\beta_0 +\\beta_1x+\\beta_2x^2+\\beta_3x^3+\\sum^k_{j=1}\\gamma_j(x-\\eta_j)^3_{+}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ca8e26",
   "metadata": {
    "Rmd_chunk_options": "echo=FALSE, fig.width=5,fig.height=4 ,out.width=\"80%\",message=FALSE",
    "jupyter": {
     "output_hidden": false,
     "source_hidden": true
    },
    "kernel": "Python3",
    "tags": [
     "report_output"
    ]
   },
   "outputs": [],
   "source": [
    "# Fit the linear regression model\n",
    "lm1 = sm.formula.ols(formula='yg ~ bs(x, knots=[0.5], degree=3)', data=dt).fit()\n",
    "\n",
    "# Create a data frame for prediction\n",
    "newdat = pd.DataFrame({'x': np.arange(dt['x'].min(), dt['x'].max(), 0.01)})\n",
    "\n",
    "# Predict\n",
    "newdat['yg'] = lm1.predict(newdat)\n",
    "## Plot the previous plot with a regression line\n",
    "(\n",
    "ggplot(data = dt,\n",
    "                mapping = aes(x = \"x\", y = \"yg\")) +geom_point(alpha=0.3) + geom_line(data = newdat, color = \"red\")+ stat_function(fun=lambda x: np.log((x)**2-x+1),color=\"blue\",alpha=0.7,linetype=\"dashed\")     +geom_vline(xintercept=0.5,linetype=\"dotted\")\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d832baac",
   "metadata": {},
   "source": [
    "#### Interpolating Splines\n",
    "Suppose we know the values of a function at $k$ points $x_1 < \\dots < x_k$ and would like to interpolate for other $x$’s.\n",
    "If we used a spline of degree $m$ with knots at the observed $x$’s, we would have $m + 1 + k$ parameters to estimate with only $k$ observations. Obviously, we need some restrictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003fa5d5",
   "metadata": {
    "Rmd_chunk_options": "fig.width=10,fig.height=4 ,out.width=\"80%\",message=FALSE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "women=pd.read_csv(\"women.csv\")\n",
    "(\n",
    "ggplot(women)+aes(x=\"height\",y=\"weight\")+geom_point()\n",
    ")\n",
    "(\n",
    "ggplot(dti)+aes(x=\"xi\",y=\"yi\")+geom_point()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644dd032",
   "metadata": {},
   "source": [
    "#### Natural Splines\n",
    "A spline of odd degree $m = 2\\nu-1$ is called a natural spline if it is a polynomial of degree $\\nu - 1$ outside the range of the knots (i.e. below $\\eta_1$ or above $\\eta_k$).\n",
    "A natural cubic spline ($\\nu=2$) is linear outside the range of the data.\n",
    "For a natural spline,\n",
    "\\begin{eqnarray*}\n",
    "\\beta_j &=& 0 \\mbox{ for } j=\\nu,\\dots,2\\nu-1 \\\\\n",
    "\\sum^k_{i=1}\\gamma_i\\eta_i^j &=&0 \\mbox{ for } j=0,1,\\dots,\\nu-1.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "This imposes exactly $m + 1$ restrictions, so we have $k$ parameters left. Note that a natural cubic spline has the form\n",
    "\n",
    "$$S(x)=\\beta_0 +\\beta_1 x+ \\sum^k_{j=1}\\gamma_j(x-\\eta_j)^3_{+}$$,\n",
    "subject to the restrictions\n",
    "$\\sum \\gamma_j =0$ and $\\sum \\gamma_j\\eta_j =0$\n",
    "so we end up with $k$ parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16010bd",
   "metadata": {
    "Rmd_chunk_options": "fig.width=10,fig.height=4 ,out.width=\"80%\",message=FALSE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# library(splines)\n",
    "# require(graphics); require(stats)\n",
    "# ispl  <- interpSpline( women$height, women$weight ,bSpline=TRUE)\n",
    "# ispl2 <- interpSpline( weight ~ height,  women ,bSpline=TRUE)\n",
    "# # ispl and ispl2 should be the same\n",
    "# par(mfrow = c(1,2), mgp = c(2,.8,0), mar = 0.1+c(3,3,3,1))\n",
    "#\n",
    "# #par(mfrow=c(1,2))\n",
    "# plot( predict( ispl, seq( 55, 75, length.out = 51 ) ), type = \"l\" )\n",
    "# points( women$height, women$weight )\n",
    "# #plot( ispl )    # plots over the range of the knots\n",
    "# #points( women$height, women$weight )\n",
    "# #splineKnots( ispl )\n",
    "#\n",
    "# ispl2 <- interpSpline( yi ~ xi ,bSpline=TRUE)\n",
    "# plot(predict( ispl2, seq( -1, 10, length.out = 51 ) ), type = \"l\")\n",
    "# points(xi,yi)\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "# Define the interpolation function using cubic splines\n",
    "ispl = interp1d(women['height'], women['weight'], kind='cubic')\n",
    "\n",
    "# Plot the interpolated values\n",
    "x_values = np.linspace(58, 72, 51)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x_values, ispl(x_values), 'r-', label='Interpolated spline')\n",
    "plt.scatter(women['height'], women['weight'], color='blue', label='Original data')\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Weight')\n",
    "plt.title('Interpolation using Cubic Splines')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Define the interpolation function using cubic splines\n",
    "ispl2 = interp1d(xi, yi, kind='cubic')\n",
    "\n",
    "# Plot the interpolated values\n",
    "x_values = np.linspace(1, 9, 51)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x_values, ispl2(x_values), 'r-', label='Interpolated spline')\n",
    "plt.scatter(xi, yi, color='blue', label='Original data')\n",
    "plt.xlabel('xi')\n",
    "plt.ylabel('yi')\n",
    "plt.title('Interpolation using Cubic Splines')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd2b685",
   "metadata": {},
   "source": [
    "#### Spline Regression\n",
    "\n",
    "Consider now the problem of smoothing a scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44d8768",
   "metadata": {
    "Rmd_chunk_options": "fig.width=10,fig.height=4 ,out.width=\"80%\",message=FALSE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#Auto = load_data('Auto')\n",
    "(\n",
    "ggplot(dt)+aes(x=\"x\",y=\"yg\")+geom_point()\n",
    ")\n",
    "(\n",
    "ggplot(Auto)+aes(x=\"mpg\",y=\"acceleration\")+geom_point()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558daad9",
   "metadata": {},
   "source": [
    "One approach is to select s suitable set of knots with $k << n$ and then fit a spline by OLS (or WLS, or maximum likelihood).\n",
    "\n",
    "For a cubic spline, this amounts to regressing $y$ on $k + 4$ predictors, namely\n",
    "$1,x,x_2,x_3,(x-\\eta_1)^3_{+},(x-\\eta_2)^3_{+},...,(x-\\eta_k)^3_{+}$\n",
    "For a natural cubic spline, we would drop $x^2$ and $x^3$ and impose the additional constraints\n",
    "  $$\\sum\\gamma =  \\sum \\gamma\\eta = 0.$$\n",
    "Actually, these constraints can be eliminated by suitable re-parametrization. For example, a natural cubic spline with two interior knots plus one knot at each extreme of the data can be fit by regressing $y$ on three covariates, $x$, $z_1$, and $z_2$, where\n",
    " and\n",
    "$$z_1 = ( x - \\eta_1 )^3_{+} -\\frac{ ( \\eta_1 - \\eta_4 )} {(\\eta_3 - \\eta_4)} ( x - \\eta_3 )^3_{+}$$\n",
    "$$z_2 = ( x - \\eta_2 )^3_{+} -\\frac{( \\eta_2 - \\eta_4 )} {(\\eta_3 - \\eta_4)}( x - \\eta_3 )^3_{+} .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08428e4",
   "metadata": {},
   "source": [
    "#### B-Splines\n",
    "\n",
    "A much better representation of splines for computation is as linear combinations of a set of basis splines called B-splines. These are numerically more stable, among other reasons, because each B-spline is non-zero over a limited range of knots. They are not so easy to calculate, but fortunately, R and S have functions for calculating a basis, see bs for B-splines and ns for natural B-splines.\n",
    "Regression splines are very popular because they are easy to use, and can be incorporated without difficulty as part of other estimation procedures.\n",
    "The main problem is where to place the knots. Often, they are placed at selected percentiles. A smarter strategy would place more knots in regions where $f(x)$ is changing more rapidly. Knot placement is an arcane art form and the first disadvantage cited by detractors of regression splines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcfe2fd",
   "metadata": {
    "Rmd_chunk_options": "fig.width=10,fig.height=4 ,out.width=\"80%\",message=FALSE",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "(\n",
    "  ggplot(dt)+aes(x=\"x\",y=\"yg\")+geom_point()\n",
    ")\n",
    "(\n",
    "  ggplot(Auto)+aes(x=\"mpg\",y=\"acceleration\")+geom_point()\n",
    ")\n",
    "#\n",
    "# Generate a sequence of xxmpg values\n",
    "xxmpg = np.arange(9, 46.7, 0.1)\n",
    "\n",
    "# Fit the linear regression model with a cubic spline term\n",
    "model = sm.formula.ols(formula='acceleration ~ bs(mpg, df=5)', data=Auto).fit()\n",
    "\n",
    "# Create a DataFrame for prediction\n",
    "newdata = pd.DataFrame({'mpg': xxmpg})\n",
    "\n",
    "# Make predictions\n",
    "predac = model.predict(newdata)\n",
    "\n",
    "# Plot the original data points\n",
    "plt.scatter(Auto['mpg'], Auto['acceleration'], color='blue', label='Original Data')\n",
    "\n",
    "# Plot the predicted values\n",
    "plt.plot(xxmpg, predac, color='red', label='Predicted Values')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('mpg')\n",
    "plt.ylabel('acceleration')\n",
    "plt.title('Acceleration vs. MPG with Cubic Spline')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db4ce61",
   "metadata": {},
   "source": [
    "#### Smoothing Splines\n",
    "\n",
    "A more formal approach to the problem is to consider fitting a spline with knots at every data point. It could fit perfectly but estimate its parameters by minimizing the usual sum of squares plus a roughness penalty.\n",
    "A suitable penalty is to integrate the squared second derivative, leading to the following criterion, known as the penalized sum of squares:\n",
    "\n",
    "$$PSS =  (y_i-S(x_i))^2 + \\lambda(S''(x))^2dx$$\n",
    "\n",
    "where integration is over the range of x, and $\\lambda$ is a tuning parameter. As $\\lambda\\rightarrow 0$ we impose no penalty and end up with a very close fit, but the resulting curve could be very noisy as it follows every detail in the data. As $\\lambda\\rightarrow \\infty$ the penalty dominates, the solution converges to the OLS line, which is as smooth as you can get (the second derivative is always 0) but may be a very poor fit.\n",
    "Amazingly, it can be shown that minimizing the PSS for a fixed $\\lambda$ over the space of all continuous differentiable functions leads to a unique solution, and this solution is a natural cubic spline with knots at the data points.\n",
    "More generally, penalizing the squared v-th derivative leads to a natural spline of degree $2\\nu-1$. For proof, see Reinsch (1967)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c3de38",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# smooth.spline2 <- function(formula, data, ...) {\n",
    "#   mat <- model.frame(formula, data)\n",
    "#   smooth.spline(mat[, 2], mat[, 1])\n",
    "# }\n",
    "#\n",
    "# predictdf.smooth.spline <- function(model, xseq, se, level) {\n",
    "#   pred <- predict(model, xseq)\n",
    "#   data.frame(x = xseq, y = pred$y)\n",
    "# }\n",
    "#\n",
    "# qplot(mpg, wt, data = mtcars) + geom_smooth(method = \"smooth.spline2\", se= F)\n",
    "# qplot(x, yg, data = dt) + geom_smooth(method = \"smooth.spline2\", se= F)\n",
    "\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "def smooth_spline(x, y):\n",
    "    spline = UnivariateSpline(x, y)\n",
    "    return spline(x)\n",
    "\n",
    "def predict_df_smooth_spline(model, xseq):\n",
    "    y_pred = model(xseq)\n",
    "    return pd.DataFrame({'x': xseq, 'y': y_pred})\n",
    "\n",
    "# Example usage\n",
    "mtcars = pd.read_csv('mtcars.csv')  # Assuming mtcars.csv contains the data\n",
    "#mtcars = pd.read_csv('https://gist.githubusercontent.com/ZeccaLehn/4e06d2575eb9589dbe8c365d61cb056c/raw/64f1660f38ef523b2a1a13be77b002b98665cdfe/mtcars.csv')\n",
    "# Edit element of column header\n",
    "mtcars.rename(columns={'Unnamed: 0':'brand'}, inplace=True)\n",
    "mtcars=mtcars.sort_values(by=['mpg'])\n",
    "#dt = pd.read_csv('dt.csv')  # Assuming dt.csv contains the data\n",
    "\n",
    "# Plotting with smooth.spline2 method\n",
    "fig, axes = plt.subplots(2)\n",
    "\n",
    "# First plot\n",
    "axes[0].scatter(mtcars['mpg'], mtcars['wt'])\n",
    "axes[0].plot(mtcars['mpg'], smooth_spline(mtcars['mpg'], mtcars['wt']), color='red')\n",
    "axes[0].set_xlabel('mpg')\n",
    "axes[0].set_ylabel('wt')\n",
    "\n",
    "# Second plot\n",
    "axes[1].scatter(dt['x'], dt['yg'])\n",
    "axes[1].plot(dt['x'], smooth_spline(dt['x'], dt['yg']), color='red')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('yg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c3585",
   "metadata": {},
   "source": [
    "#### Cross-Validation\n",
    "We have solved the problem of knot placement, but now we have to pick an appropriate value for $\\lambda$. Some claim this is easier because we are left with a single number to worry about.\n",
    "Wabba and others have suggested a technique known as cross-validation. Let $S_{\\lambda}(-i)$ denote the spline fit with tuning parameter $\\lambda$ while omitting the i-th observation. We can compare this fit with the observed value $y_i$, and\n",
    "we can summarize these differences by computing a sum of squares\n",
    "$$CVSS(\\lambda) = \\sum_{i=1}^{n} (y_i-\\hat{S}_{\\lambda} (x_i))^2 $$\n",
    " which depends on $\\lambda$. The idea is to pick $\\lambda$ to minimize the $CVSS(\\lambda)$.\n",
    "This sounds like a lot of work but it isn’t, thanks again to regression updating formulas, which can be used to show that\n",
    "$$CVSS(\\lambda) = \\sum_{i=1}^{n} \\frac{(y_i - \\hat{S}_{\\lambda} (x_i))^2}{ 1 - A_{ii}}$$\n",
    "\n",
    "where $A_{ii}$ is a diagonal element of $A = (I - \\lambda K)-1$. This extends easily to WLS.\n",
    "An alternative criterion is to replace the Aii with their average, which is $tr(A)/n$. This leads to a generalized CVSS that has been found to work well in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738d6b53",
   "metadata": {},
   "source": [
    "### basis function plotting\n",
    "\n",
    "Basis functions are mysterious, but it's pretty neat once you get what bs and ns are doing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0aef7a",
   "metadata": {},
   "source": [
    "#### linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43b0696",
   "metadata": {
    "Rmd_chunk_options": "echo=show_code",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# from ISLP import load_data\n",
    "Wage = load_data('Wage')\n",
    "# fit1 = smf.ols('wage~bs(age, knots = (25,40,60), degree = 1, include_intercept = False)',\n",
    "#                            data = Wage).fit()\n",
    "#\n",
    "# #fit1=lm(wage~bs(age,degree=1,knots=c(25,40,60)),data=Wage)\n",
    "# fit1.params\n",
    "#\n",
    "# bs.weight1<-fit1$coefficients[-1]/sum(fit1$coefficients[-1])\n",
    "# bs.age1<-with(Wage,bs(age,degree=1,knots=c(25,40,60)))\n",
    "# xage=seq(18,80,by=1)\n",
    "# pred.bs.age1<- predict(bs.age1,newx=xage)\n",
    "\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrix\n",
    "\n",
    "# Assuming Wage is a DataFrame containing the data\n",
    "\n",
    "# Fit linear regression with basis splines\n",
    "knots = [25, 40, 60]\n",
    "bs_age = dmatrix(\"bs(age, knots=knots, degree=1, include_intercept=False)\", {\"age\": Wage[\"age\"]})\n",
    "X = sm.add_constant(bs_age)\n",
    "y = Wage[\"wage\"]\n",
    "fit1 = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print coefficients\n",
    "print(fit1.params)\n",
    "\n",
    "# Calculate weights for basis splines\n",
    "bs_weight1 = fit1.params[1:] / sum(fit1.params[1:])\n",
    "\n",
    "# Generate sequence of ages\n",
    "xage = np.arange(18, 81, 1)\n",
    "\n",
    "# Predict using basis splines\n",
    "pred_bs_age1 = dmatrix(\"bs(xage, knots=knots, degree=1, include_intercept=False)\", {\"xage\": xage})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccfb4e6",
   "metadata": {
    "Rmd_chunk_options": "echo=show_code",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# par(mfrow=c(1,3))\n",
    "# plot(range(xage),c(0,1),type=\"n\")\n",
    "# for(i in 1:4) lines(xage,pred.bs.age1[,i])\n",
    "# plot(range(xage),c(0,1),type=\"n\")\n",
    "# for(i in 1:4) lines(xage,bs.weight1[i]*pred.bs.age1[,i])\n",
    "# plot(Wage$age,Wage$wage,col=\"gray\")\n",
    "# lines(xage,fit1$coefficients[1]+pred.bs.age1%*%fit1$coefficients[-1],type=\"l\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot basis splines\n",
    "axes[0].set_xlim(min(xage), max(xage))\n",
    "axes[0].set_ylim(0, 1)\n",
    "for i in range(pred_bs_age1.shape[1]):\n",
    "    axes[0].plot(xage, pred_bs_age1[:, i])\n",
    "\n",
    "# Plot weighted basis splines\n",
    "axes[1].set_xlim(min(xage), max(xage))\n",
    "axes[1].set_ylim(0, 1)\n",
    "for i in range(bs_weight1.shape[0]):\n",
    "    axes[1].plot(xage, bs_weight1[i] * pred_bs_age1[:, i])\n",
    "\n",
    "# Plot age vs wage with the linear regression fit\n",
    "axes[2].scatter(Wage['age'], Wage['wage'], color=\"gray\")\n",
    "axes[2].plot(xage, fit1.params[0] + np.dot(pred_bs_age1[:,1:5], fit1.params[1:]), color='blue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebaf6c9",
   "metadata": {},
   "source": [
    "#### quadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2cbd02",
   "metadata": {
    "Rmd_chunk_options": "echo=show_code",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# fit2=lm(wage~bs(age,degree=2,knots=c(25,40,60)),data=Wage)\n",
    "# bs.weight2<-fit2$coefficients[-1]/sum(fit2$coefficients[-1])\n",
    "# bs.age2<-with(Wage,bs(age,degree=2,knots=c(25,40,60)))\n",
    "# xage=seq(18,80,by=1)\n",
    "# pred.bs.age2<- predict(bs.age2,newx=xage)\n",
    "\n",
    "# par(mfrow=c(1,3))\n",
    "# plot(range(xage),c(0,1),type=\"n\")\n",
    "# for(i in 1:5) lines(xage,pred.bs.age2[,i])\n",
    "# plot(range(xage),c(0,1),type=\"n\")\n",
    "# for(i in 1:5) lines(xage,bs.weight2[i]*pred.bs.age2[,i])\n",
    "# plot(Wage$age,Wage$wage,col=\"gray\")\n",
    "# lines(xage,fit2$coefficients[1]+pred.bs.age2%*%fit2$coefficients[-1],type=\"l\")\n",
    "\n",
    "\n",
    "\n",
    "# Fit linear regression with quadratic splines\n",
    "bs_age2 = dmatrix(\"bs(age, knots=knots, degree=2, include_intercept=False)\", {\"age\": Wage[\"age\"]})\n",
    "X2 = sm.add_constant(bs_age2)\n",
    "fit2 = sm.OLS(y, X2).fit()\n",
    "\n",
    "# Calculate weights for quadratic splines\n",
    "bs_weight2 = fit2.params[1:] / sum(fit2.params[1:])\n",
    "\n",
    "# Generate sequence of ages\n",
    "xage = np.arange(18, 81, 1)\n",
    "\n",
    "# Predict using quadratic splines\n",
    "pred_bs_age2 = dmatrix(\"bs(xage, knots=knots, degree=2, include_intercept=False)\", {\"xage\": xage})\n",
    "\n",
    "# Set up multi-panel plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot basis splines\n",
    "axes[0].set_xlim(min(xage), max(xage))\n",
    "axes[0].set_ylim(0, 1)\n",
    "for i in range(1,pred_bs_age2.shape[1]):\n",
    "    axes[0].plot(xage, pred_bs_age2[:, i])\n",
    "\n",
    "\n",
    "# Plot weighted basis splines\n",
    "axes[1].set_xlim(min(xage), max(xage))\n",
    "axes[1].set_ylim(0, 1)\n",
    "for i in range(1,pred_bs_age2.shape[1]):\n",
    "    axes[1].plot(xage, bs_weight2[i-1] * pred_bs_age2[:, i])\n",
    "\n",
    "\n",
    "# Plot age vs wage with the linear regression fit\n",
    "axes[2].scatter(Wage['age'], Wage['wage'], color=\"gray\")\n",
    "axes[2].plot(xage, fit2.params[0] + np.dot(pred_bs_age2[:,1:6], fit2.params[1:]), color='blue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6117f628",
   "metadata": {},
   "source": [
    "#### cubic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce908d8",
   "metadata": {
    "Rmd_chunk_options": "echo=show_code",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# fit3=lm(wage~bs(age,knots=c(25,40,60)),data=Wage)\n",
    "# bs.age<-with(Wage,bs(age,knots=c(25,40,60)))\n",
    "# xage=seq(18,80,by=1)\n",
    "# pred.bs.age<- predict(bs.age,newx=xage)\n",
    "# par(mfrow=c(1,3))\n",
    "# plot(range(xage),c(0,1),type=\"n\")\n",
    "# for(i in 1:6) lines(xage,pred.bs.age[,i])\n",
    "# plot(range(xage),c(0,1),type=\"n\")\n",
    "# for(i in 1:6) lines(xage,bs.weight1[i]*pred.bs.age[,i])\n",
    "# plot(Wage$age,Wage$wage,col=\"gray\")\n",
    "# lines(xage,fit3$coefficients[1]+pred.bs.age%*%fit3$coefficients[-1],type=\"l\")\n",
    "\n",
    "# Fit linear regression with linear splines\n",
    "bs_age = dmatrix(\"bs(age, knots=knots, include_intercept=False)\", {\"age\": Wage[\"age\"]})\n",
    "X3 = sm.add_constant(bs_age)\n",
    "fit3 = sm.OLS(y, X3).fit()\n",
    "bs_weight3 = fit3.params[1:] / sum(fit3.params[1:])\n",
    "\n",
    "# Generate sequence of ages\n",
    "xage = np.arange(18, 81, 1)\n",
    "\n",
    "# Predict using linear splines\n",
    "pred_bs_age = dmatrix(\"bs(xage, knots=knots, include_intercept=False)\", {\"xage\": xage})\n",
    "\n",
    "# Set up multi-panel plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot basis splines\n",
    "axes[0].set_xlim(min(xage), max(xage))\n",
    "axes[0].set_ylim(0, 1)\n",
    "for i in range(1,pred_bs_age.shape[1]):\n",
    "    axes[0].plot(xage, pred_bs_age[:, i])\n",
    "\n",
    "# Plot weighted basis splines\n",
    "axes[1].set_xlim(min(xage), max(xage))\n",
    "axes[1].set_ylim(0, 1)\n",
    "for i in range(1,pred_bs_age.shape[1]):\n",
    "    axes[1].plot(xage, bs_weight3[i-1] * pred_bs_age[:, i])\n",
    "\n",
    "# Plot age vs wage with the linear regression fit\n",
    "axes[2].scatter(Wage['age'], Wage['wage'], color=\"gray\")\n",
    "axes[2].plot(xage, fit3.params[0] + np.dot(pred_bs_age[:,1:7], fit3.params[1:]), color='blue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52824ba8",
   "metadata": {},
   "source": [
    "### Manually drawing the interval\n",
    "\n",
    "If you want to draw confidence and prediction intervals, using the R predict function is the easiest thing to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc705c0",
   "metadata": {
    "Rmd_chunk_options": "fig.width=6,fig.height=5 ,out.width=\"60%\"",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from ISLP.models import ModelSpec as MS, ns\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrix\n",
    "# Data Generation\n",
    "np.random.seed(12345)\n",
    "x = np.arange(1, 101)\n",
    "y = np.sin(np.pi * x / 50) + np.random.normal(0, 0.4, 100)\n",
    "dtaa = pd.DataFrame({'x': x, 'y': y})\n",
    "epsilon = np.random.normal(0, 3, 100)\n",
    "knots = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "\n",
    "ns_x = MS([ns('x', df=5)]).fit(dtaa)\n",
    "myFit = sm.OLS(y, ns_x.transform(dtaa)).fit()\n",
    "\n",
    "# Plotting the Result\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, label='Data')\n",
    "plt.plot(x, myFit.fittedvalues, label='Fitted', color='red')\n",
    "plt.plot(x, myFit.get_prediction().conf_int()[:, 0], linestyle='--', color='blue', label='Confidence Interval')\n",
    "plt.plot(x, myFit.get_prediction().conf_int()[:, 1], linestyle='--', color='blue')\n",
    "plt.plot(x, myFit.get_prediction().predicted_mean, linestyle='--', color='green', label='Prediction Interval')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3e43ad",
   "metadata": {
    "Rmd_chunk_options": "fig.width=6,fig.height=5 ,out.width=\"60%\"",
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Point-wise Standard Error Calculation\n",
    "X = sm.add_constant(x)\n",
    "sigma = myFit.mse_resid ** 0.5\n",
    "var_Yhat = np.diag(np.dot(X, np.dot(np.linalg.inv(np.dot(X.T, X)), X.T))) * sigma ** 2\n",
    "print(np.mean(myFit.get_prediction().se_mean - np.sqrt(var_Yhat)))\n",
    "\n",
    "# Another Option for Standard Error Calculation\n",
    "# X_new = np.column_stack([np.ones(101), ns(np.arange(50, 151), knots=knots, intercept=False)])\n",
    "# var_Yhat = (np.diag(np.dot(X_new, np.dot(np.linalg.inv(np.dot(X.T, X)), X_new.T))) + 1) * sigma ** 2\n",
    "# print(np.mean(myFit.get_prediction(exog=X_new).se_mean - np.sqrt(var_Yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feff415",
   "metadata": {},
   "source": [
    "### using mgcv gam\n",
    "\n",
    "There are two popular packages for fitting GAM in R.\n",
    "\n",
    "- `gam`\n",
    "- `mgcv`\n",
    "\n",
    "The authors of the book like `gam`.  But `mgcv` is better at doing some things because they are Bayesian. The key difference is that `gam` uses smoothing spline whereas `mgcv` uses p-spline.  Also, the uncertainty interval is Bayesian for `mgcv`, which tends to be better calibrated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56c6e48",
   "metadata": {},
   "source": [
    "#### kyphosis example (gam)\n",
    "\n",
    "84 Children at the Toronto Hospital for Sick Children underwent Laminectomy, a corrective spinal surgery for a variety of abnormalities under the general heading kyphosis.\n",
    "Results: 65 successes, 19 kyphosis still present.\n",
    "Goal: Try to understand/predict whether the operation will be successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d89188",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "kyphosis=pd.read_csv(\"kyphosis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f85401",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "kyphosis['Kyphosis01'] = kyphosis['Kyphosis'].apply(lambda x: 1 if x == \"present\" else 0)\n",
    "\n",
    "\n",
    "from pygam import LogisticGAM, s\n",
    "# Fit the Generalized Additive Model (GAM)\n",
    "kyphosis_gam = LogisticGAM(s(0) + s(1, n_splines=5) + s(2, n_splines=5),\n",
    "                            fit_intercept=True).fit(kyphosis[['Age', 'Number', 'Start']], kyphosis['Kyphosis01'])\n",
    "\n",
    "# Display the summary of the GAM\n",
    "print(kyphosis_gam.summary())\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "titles = ['Age', 'Number', 'Start']\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    XX = kyphosis_gam.generate_X_grid(term=i)\n",
    "    pdep, confi = kyphosis_gam.partial_dependence(term=i, width=.95)\n",
    "    ax.plot(XX[:, i], pdep)\n",
    "    ax.plot(XX[:, i], confi, c='r', ls='--')\n",
    "    ax.set_title(titles[i]);\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fd2b01",
   "metadata": {},
   "source": [
    "### Other Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f937ea",
   "metadata": {},
   "source": [
    "#### Air pollution example (HSAUR2)\n",
    "Air pollution data of 41 US cities. The annual mean concentration of sulfur dioxide, in micrograms per cubic meter, is a measure of the city's air pollution. The question of interest here is what aspects of climate and human ecology, as measured by the other six variables in the data, determine pollution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d44704",
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from pygam import LinearGAM\n",
    "\n",
    "USairpollution=pd.read_csv(\"USairpollution.csv\")\n",
    "USairpollution.rename(columns={\"Unnamed: 0\": \"State\"})\n",
    "# Fit the Generalized Additive Model (GAM)\n",
    "USairpollution_gam = LinearGAM(s(0) + s(1) + s(2) + s(3) + s(4) + s(5)).fit(USairpollution.loc[:,[\"temp\",  \"manu\" , \"popul\"  ,\"wind\"  ,\"precip\" , \"predays\"]], USairpollution['SO2'])\n",
    "\n",
    "# Predict SO2\n",
    "SO2hat = USairpollution_gam.predict(USairpollution.loc[:,[\"temp\",  \"manu\" , \"popul\"  ,\"wind\"  ,\"precip\" , \"predays\"]])\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = USairpollution['SO2'] - SO2hat\n",
    "\n",
    "# Create the residual plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(SO2hat, residuals, 'bo', markersize=5)\n",
    "plt.axhline(y=0, color='grey', linestyle='--')\n",
    "plt.xlabel('Fitted values (SO2hat)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(1, 6)\n",
    "titles = [\"temp\",  \"manu\" , \"popul\"  ,\"wind\"  ,\"precip\" , \"predays\"]\n",
    "xxp=USairpollution.loc[:,[\"temp\",  \"manu\" , \"popul\"  ,\"wind\"  ,\"precip\" , \"predays\"]]\n",
    "for i, ax in enumerate(axs):\n",
    "    XX = USairpollution_gam.generate_X_grid(term=i)\n",
    "    pdep= USairpollution_gam.predict(XX)\n",
    "    confi = USairpollution_gam.prediction_intervals(XX, width=.95)\n",
    "    ax.scatter(xxp[titles[i]], USairpollution['SO2'], c='k', alpha=0.2)\n",
    "    ax.plot(XX[:, i], pdep)\n",
    "    ax.plot(XX[:, i], confi, c='r', ls='--')\n",
    "    ax.set_title(titles[i]);\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "Rmd_chunk_options": {
   "author": "Your Name",
   "date": "2024-01-05",
   "output": "html_document",
   "title": "Chapter 7 Lab: Non-linear Modeling"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ],
    [
     "R",
     "ir",
     "",
     ""
    ],
    [
     "css",
     "css",
     "",
     ""
    ],
    [
     "Python3",
     "ir",
     "",
     ""
    ]
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
